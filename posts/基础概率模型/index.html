<!DOCTYPE html>
<html><meta charset="utf-8">
<meta name="renderer" content="webkit">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">


<meta name="keywords" content="">
<meta name="description" content="">

<title>基础概率模型</title>


<meta name="generator" content="Hugo 0.102.3" />




<link rel="stylesheet" href="/css/main.min.css">
<div class="fixed-header-slide">
    <div class="fixed-header-wrap">
        <header class="header">
            <nav class="nav">
                <div class="logo">
                    <a href="http://localhost:1313/" title="Lzero的小站">Lzero的小站</a>
                </div>
                <ul class="menu" id="menu" onscroll="menu_on_scroll()">
                    <li>
                        <a href="http://localhost:1313/about" title="About">
                            <span>About</span>
                        </a>
                    </li>
                    <li>
                        <a href="http://localhost:1313/archives" title="Archive">
                            <span>Archive</span>
                        </a>
                    </li>
                    <li>
                        <a href="http://localhost:1313/brochures" title="Brochure">
                            <span>Brochure</span>
                        </a>
                    </li></ul>
            </nav>
        </header>
    </div>
</div><body>
		<main class="content">







  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3&#43;Aro6EYUG4&#43;cU&#43;KJWu/X" crossorigin="anonymous">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c&#43;Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI&#43;sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC&#43;Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"></script>


<script>
  document.addEventListener('DOMContentLoaded', function () {
    'use strict';
    
    
    var tables = document.querySelectorAll('.post-body > table');
    for (let i = 0; i < tables.length; i++) {
      var table = tables[i];
      var wrapper = document.createElement('div');
      wrapper.className = 'table-wrapper';
      table.parentElement.replaceChild(wrapper, table);
      wrapper.appendChild(table);
    }
    


    
    
      var mathElements = document.getElementsByClassName('math');
      var options = {
        delimiters: [
          { left: "$$", right: "$$", display: true },
          { left: "$$\n", right: "\n$$", display: true },
          { left: "$", right: "$", display: false },
        ],
      };

      renderMathInElement(document.querySelector('.post-body'), options);
    
    
  });
</script>


<div class="wrapper-toc">
    <div class="toc">
		
		
		
			<div class="toc-content">
			
				
				
				
				<label style="margin: auto;">-Catalogue-</label>
				
				
				<ul></ul>
					
						
						
							
								
								
								
								
									
										<ul>
									
								
								
									<li>
										<a href="#1%e8%b4%9d%e5%8f%b6%e6%96%af%e5%ae%9a%e7%90%86" onclick="onNavClick(`#1贝叶斯定理-nav`)" id="1贝叶斯定理-nav" class="animated-visibility">
											1、贝叶斯定理
										</a>
									</li>
								
								
							
						
					
						
						
							
								
								
								
								
								
									<li>
										<a href="#2%e6%9c%b4%e7%b4%a0%e8%b4%9d%e5%8f%b6%e6%96%af" onclick="onNavClick(`#2朴素贝叶斯-nav`)" id="2朴素贝叶斯-nav" class="animated-visibility">
											2、朴素贝叶斯
										</a>
									</li>
								
								
							
						
					
						
						
							
								
								
								
								
									
										<ul>
									
								
								
									<li>
										<a href="#21-%e6%9c%b4%e7%b4%a0%e8%b4%9d%e5%8f%b6%e6%96%af%e5%88%86%e7%b1%bb%e8%bf%87%e7%a8%8b" onclick="onNavClick(`#21-朴素贝叶斯分类过程-nav`)" id="21-朴素贝叶斯分类过程-nav" class="animated-visibility">
											2.1 朴素贝叶斯分类过程：
										</a>
									</li>
								
								
							
						
					
						
						
							
								
								
								
								
								
									<li>
										<a href="#22-%e4%b8%8e%e5%85%b6%e4%bb%96%e5%88%86%e7%b1%bb%e7%ae%97%e6%b3%95%e7%9a%84%e6%af%94%e8%be%83" onclick="onNavClick(`#22-与其他分类算法的比较-nav`)" id="22-与其他分类算法的比较-nav" class="animated-visibility">
											2.2 与其他分类算法的比较
										</a>
									</li>
								
								
							
						
					
						
						
							
								
								
								
									
										</ul>
									
								
								
								
									<li>
										<a href="#3%e9%a9%ac%e5%b0%94%e5%8f%af%e5%a4%abmarkov%e5%81%87%e8%ae%be" onclick="onNavClick(`#3马尔可夫markov假设-nav`)" id="3马尔可夫markov假设-nav" class="animated-visibility">
											3、马尔可夫(Markov)假设
										</a>
									</li>
								
								
							
						
					
						
						
							
								
								
								
								
									
										<ul>
									
								
								
									<li>
										<a href="#31-%e9%93%be%e5%bc%8f%e6%b3%95%e5%88%99" onclick="onNavClick(`#31-链式法则-nav`)" id="31-链式法则-nav" class="animated-visibility">
											3.1 链式法则
										</a>
									</li>
								
								
							
						
					
						
						
							
								
								
								
								
								
									<li>
										<a href="#32-%e9%a9%ac%e5%b0%94%e5%8f%af%e5%a4%abmarkov%e5%81%87%e8%ae%be" onclick="onNavClick(`#32-马尔可夫markov假设-nav`)" id="32-马尔可夫markov假设-nav" class="animated-visibility">
											3.2 马尔可夫(Markov)假设
										</a>
									</li>
								
								
							
						
					
						
						
							
								
								
								
								
								
									<li>
										<a href="#33-%e5%a4%9a%e5%85%83%e6%96%87%e6%b3%95ngram%e6%a8%a1%e5%9e%8b" onclick="onNavClick(`#33-多元文法ngram模型-nav`)" id="33-多元文法ngram模型-nav" class="animated-visibility">
											3.3 多元文法(Ngram)模型
										</a>
									</li>
								
								
							
						
					
						
						
							
								
								
								
									
										</ul>
									
								
								
								
									<li>
										<a href="#4%e9%a9%ac%e5%b0%94%e5%8f%af%e5%a4%ab%e6%a8%a1%e5%9e%8b" onclick="onNavClick(`#4马尔可夫模型-nav`)" id="4马尔可夫模型-nav" class="animated-visibility">
											4、马尔可夫模型
										</a>
									</li>
								
								
							
						
					
						
						
							
								
								
								
								
								
									<li>
										<a href="#5%e4%bf%a1%e6%81%af%e7%86%b5" onclick="onNavClick(`#5信息熵-nav`)" id="5信息熵-nav" class="animated-visibility">
											5、信息熵
										</a>
									</li>
								
								
							
						
					
						
						
							
								
								
								
								
									
										<ul>
									
								
								
									<li>
										<a href="#51-%e4%bf%a1%e6%81%af%e9%87%8f%e6%9c%9f%e6%9c%9b" onclick="onNavClick(`#51-信息量期望-nav`)" id="51-信息量期望-nav" class="animated-visibility">
											5.1 信息量期望
										</a>
									</li>
								
								
							
						
					
						
						
							
								
								
								
								
								
									<li>
										<a href="#52-%e4%bf%a1%e6%81%af%e5%a2%9e%e7%9b%8ainformation-gain" onclick="onNavClick(`#52-信息增益information-gain-nav`)" id="52-信息增益information-gain-nav" class="animated-visibility">
											5.2 信息增益(Information Gain)
										</a>
									</li>
								
								
							
						
					
						
						
							
								
								
								
									
										</ul>
									
								
								
								
									<li>
										<a href="#6%e5%86%b3%e7%ad%96%e6%a0%91" onclick="onNavClick(`#6决策树-nav`)" id="6决策树-nav" class="animated-visibility">
											6、决策树
										</a>
									</li>
								
								
							
						
					
						
						
							
								
								
								
								
									
										<ul>
									
								
								
									<li>
										<a href="#61-%e4%bf%a1%e6%81%af%e7%86%b5%e4%b8%8e%e5%88%86%e7%b1%bb" onclick="onNavClick(`#61-信息熵与分类-nav`)" id="61-信息熵与分类-nav" class="animated-visibility">
											6.1 信息熵与分类
										</a>
									</li>
								
								
							
						
					
						
						
							
								
								
								
								
								
									<li>
										<a href="#62-%e5%86%b3%e7%ad%96%e6%a0%91%e8%ae%ad%e7%bb%83" onclick="onNavClick(`#62-决策树训练-nav`)" id="62-决策树训练-nav" class="animated-visibility">
											6.2 决策树训练
										</a>
									</li>
								
								
							
						
					
						
						
							
								
								
								
								
								
									<li>
										<a href="#63-%e5%87%a0%e7%a7%8d%e5%86%b3%e7%ad%96%e6%a0%91%e7%ae%97%e6%b3%95%e7%9a%84%e5%bc%82%e5%90%8c" onclick="onNavClick(`#63-几种决策树算法的异同-nav`)" id="63-几种决策树算法的异同-nav" class="animated-visibility">
											6.3 几种决策树算法的异同
										</a>
									</li>
								
								
							
						
					
						
						
							
								
								
								
									
										</ul>
									
								
								
								
									<li>
										<a href="#7%e7%89%b9%e5%be%81%e9%80%89%e6%8b%a9feature-selection" onclick="onNavClick(`#7特征选择feature-selection-nav`)" id="7特征选择feature-selection-nav" class="animated-visibility">
											7、特征选择(Feature Selection)
										</a>
									</li>
								
								
							
						
					
						
						
							
								
								
								
								
									
										<ul>
									
								
								
									<li>
										<a href="#71-%e5%88%a9%e7%94%a8%e4%bf%a1%e6%81%af%e7%86%b5%e8%bf%9b%e8%a1%8c%e7%89%b9%e5%be%81%e9%80%89%e6%8b%a9" onclick="onNavClick(`#71-利用信息熵进行特征选择-nav`)" id="71-利用信息熵进行特征选择-nav" class="animated-visibility">
											7.1 利用信息熵进行特征选择
										</a>
									</li>
								
								
							
						
					
						
						
							
								
								
								
								
								
									<li>
										<a href="#72-%e5%88%a9%e7%94%a8%e5%8d%a1%e6%96%b9%e6%a3%80%e9%aa%8c%e8%bf%9b%e8%a1%8c%e7%89%b9%e5%be%81%e9%80%89%e6%8b%a9" onclick="onNavClick(`#72-利用卡方检验进行特征选择-nav`)" id="72-利用卡方检验进行特征选择-nav" class="animated-visibility">
											7.2 利用卡方检验进行特征选择
										</a>
									</li>
								
								
							
						
					
				</ul>
			</div>
		
    </div>
</div>


<div class="post-list">
	<div class="single-post">
		<div class="post-head-wrapper">
			<div class="post-title">
				基础概率模型
				
				<div class="post-meta">
    <time class="post-meta-date" itemprop="datePublished">
        2021-04-14
    </time>

    <ul class="post-meta-tags">
        #
        
        
        
            <li><a href="http://localhost:1313/tags/%E6%A6%82%E7%8E%87/" title="概率">概率</a></li>
        
            <li><a href="http://localhost:1313/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF/" title="贝叶斯">贝叶斯</a></li>
        
            <li><a href="http://localhost:1313/tags/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB/" title="马尔可夫">马尔可夫</a></li>
        
            <li><a href="http://localhost:1313/tags/%E5%86%B3%E7%AD%96%E6%A0%91/" title="决策树">决策树</a></li>
        
            <li><a href="http://localhost:1313/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" title="机器学习">机器学习</a></li>
        
    </ul>
</div>
			</div>
		</div>
        
		<div class="post-body-wrapper">
			<div class="post-body" v-pre>
				
				
				<h2 id="1贝叶斯定理">1、贝叶斯定理<a hidden class="anchor" aria-hidden="true" href="#1贝叶斯定理">#</a></h2>
<p>$P(x|y)$是条件概率，表示<strong>在$y$事件发生的情况下，$x$事件发生的概率</strong>。故，有$P(y,x)=P(x|y)\times P(y)$，表示<strong>同时发生$x,y$事件的概率</strong>=<strong>发生$y$事件的概率</strong> 乘以 <strong>在$y$事件发生的情况下发生$x$事件的概率</strong>。</p>
<p>当$x,y$事件相互独立时，就有<strong>贝叶斯公式如下：</strong>
$$
P(x|y)\times P(y)=P(y,x)=P(x,y)=P(y|x)\times P(x) \\
\Downarrow \\
P(x|y)=\frac {P(y|x)\times P(x) }{ P(y) }
$$
其中$P(x)$称为先验概率，$P(y|x)$称为似然率(根据观测到的结果数据来预估模型的参数)，$P(y)$称为边缘概率。而最终得出的结果$P(x|y)$则称为后验概率。</p>
<h2 id="2朴素贝叶斯">2、朴素贝叶斯<a hidden class="anchor" aria-hidden="true" href="#2朴素贝叶斯">#</a></h2>
<p>朴素贝叶斯模型是一种分类模型——给出一个事物的一些<strong>特征</strong>，推断出该事物分别属于各类类别的<strong>概率</strong>。
$$
P(c|f_1,f_2,&hellip;,f_i)=P(c|f_1)\times P(c|f_2)\times &hellip; \times P(c|f_i) \\
\quad \\
\Rightarrow \frac{P(f_1|c)\times P(c) }{ P(f_1) }\times \frac{P(f_2|c)\times P(c) }{ P(f_2) }\times &hellip;\times \frac{P(f_i|c)\times P(c) }{ P(f_i) }
$$
在进行贝叶斯公式的乘积计算时，常常会出现结果为0——<strong>某类别没有出现过某种特征，因此当前的似然率是0</strong>。一般会采取<strong>平滑(Smoothing)的方式</strong>处理——取一个比这个数据集里最小统计概率还要小的极小值，来代替<strong>零概率</strong>.</p>
<p>另外，在进行概率乘积计算的过程中，当特征很多时，乘积结果可能就小到计算机无法处理的地步，因此会采用一些数学方法进行转换(比如取log，将小数转化为绝对值大于1的负数)。</p>
<h3 id="21-朴素贝叶斯分类过程">2.1 朴素贝叶斯分类过程：<a hidden class="anchor" aria-hidden="true" href="#21-朴素贝叶斯分类过程">#</a></h3>
<ul>
<li><strong>准备数据</strong>：收集各类别事物的实例，归纳各类别事物的特征，并将其转化为计算机所能理解的数据。这种数据也被称为训练样本。</li>
<li><strong>建立模型</strong>：用计算机统计类别事物、事物特征出现的先验概率，以及在某个分类下某种特征出现的条件概率。这个过程也被称为基于样本的训练。</li>
<li><strong>分类新数据</strong>：对于一个新实例的特征数据，计算机根据已经建立的模型进行推导计算，得到该实例属于每个分类的概率，实现了分类的目的。这个过程也被称为预测。</li>
</ul>
<h3 id="22-与其他分类算法的比较">2.2 与其他分类算法的比较<a hidden class="anchor" aria-hidden="true" href="#22-与其他分类算法的比较">#</a></h3>
<ul>
<li>和KNN 最近邻相比，朴素贝叶斯需要更多的时间进行模型的训练，但是它在对新的数据进行分类预测的时候，通常效果更好、用时更短。</li>
<li>和决策树相比，朴素贝叶斯并不能提供一套易于人类理解的规则，但是它可以提供决策树通常无法支持的模糊分类（一个对象可以属于多个分类）。</li>
<li>和SVM 支持向量机相比，朴素贝叶斯无法直接支持连续值的输入。所以，在前面的案例中，我将连续值转化成了离散值，便于朴素贝叶斯进行处理。</li>
</ul>
<h2 id="3马尔可夫markov假设">3、马尔可夫(Markov)假设<a hidden class="anchor" aria-hidden="true" href="#3马尔可夫markov假设">#</a></h2>
<h3 id="31-链式法则">3.1 链式法则<a hidden class="anchor" aria-hidden="true" href="#31-链式法则">#</a></h3>
<p>链式法则是概率论中一个常用法则。它使用一系列条件概念率和边缘概率，来推导联合概率。
$$
P(x_1,x_2,&hellip;,x_n)=P(x_1)\times P(x_2|x_1)\times P(x_3|x_2,x_1)\times &hellip;\times P(x_n|x_1,x_2,&hellip;,x_{n-1})
$$</p>
<h3 id="32-马尔可夫markov假设">3.2 马尔可夫(Markov)假设<a hidden class="anchor" aria-hidden="true" href="#32-马尔可夫markov假设">#</a></h3>
<p>Markov假设是基于模板表示的时序模型的基础。那么什么事时序模型呢？</p>
<p>当对动态环境进行建模时，问题的状态会随时间变化。对其状态进行推理时，可以根据系统状态进行建模。用$X^{(t)}$表示系统变量（也叫模板变量）。而$X^{(t)}$从0时刻的特定状态变化到T时刻特定状态的概率为$P(X^{0:T})$。
$$
P(X^{0:T})=P(X^{(0)},X^{(1)},X^{(2)},&hellip;,X^{(T)}) \\
\quad \\
\Rightarrow P(X^{(0)})\prod^{T-1}_{0}P(X^{(t+1)}|X^{(0:t)})
$$</p>
<p>当<strong>时序模型</strong>满足<strong>Markov假设</strong>时，$P(X^{0:T})$可以进行简化。而<strong>Markov假设</strong>表示如下：
$$
X^{(t+1)}\perp X^{(0:t-1)}|X^{(t)}
$$
即$X^{(t+1)}$与$X^{(0:t-1)}$关于$X^{(t)}$条件独立。也就是说，下一状态的概率分布只由当前状态决定，在时间序列中它前面的事件均与之无关。</p>
<h3 id="33-多元文法ngram模型">3.3 多元文法(Ngram)模型<a hidden class="anchor" aria-hidden="true" href="#33-多元文法ngram模型">#</a></h3>
<p>马尔可夫假设引用到自然语言处理中，即可以得到假设：任何一个词 $w_i$ 出现的概率只和它前面的1个或N个词有关。由此可以提出<strong>多元文法（Ngram）模型</strong>。Ngram 中的&quot;<strong>N</strong>&ldquo;表示任何一个词出现的概率，只和它前面的 N-1 个词有关。</p>
<p>以二元文法模型为例，即使某个单词出现在一个很长的句子中，也只需要看前面那1个单词。公式表示为：
$$
P(w_n|w_1,w_2,&hellip;,w_{n-1})\approx P(w_n|w_{n-1})
$$
而三元文法的公式表示也就是：
$$
P(w_n|w_1,w_2,&hellip;,w_{n-1})\approx P(w_n|w_{n-1},w_{n-2})
$$</p>
<p>假设现在有一个统计样本文本$d$，$s$是一个由按照特定顺序排列的词$w_1,w_2,&hellip;,w_n$组成的句子。现在想知道根据文档$d$的统计数据，$s$在文本中出现的可能性，即 $P(s|d)=P(w_1,w_2,&hellip;,w_n|d)$。假设考虑的都是在集合$d$的情况下发生的概率，所以可以忽略$d$，即$P(s)=P(w_1,w_2,&hellip;,w_n)$。运用上述的<strong>链式法则</strong>以及<strong>马尔科夫假设</strong>，可以推导出：
$$
P(w_1,w_2,&hellip;,w_n)=P(w_1)\times P(w_2|w_1)\times P(w_3|w_2,w_1)\times &hellip;\times P(w_n|w_1,w_2,&hellip;,w_{n-1}) \\
\quad \\
\approx P(w_1)\times P(w_2|w_1)\times P(w_3|w_2,w_1)\times P(w_4|w_3,w_2)\times &hellip;.\times P(w_n|w_{n-1},w_{n-2})
$$
以上的近似是三元文法的公式，二元即是将条件概率中前提条件去掉前前一个状态。</p>
<p>语言模型在信息检索和中文分词这两个方面里都发挥着这重要作用：</p>
<ol>
<li><strong>信息检索</strong>：信息检索很关心的一个问题就是相关性，也就是说，给定一个查询，哪篇文档是更相关的呢？一种常见的做法是计算$P(d|q)$，$P(d|q)$表示用户输入查询$q$的情况下，文档$d$ 出现的概率。概率越高，$q$和$d$之间的相关性越高。
$$
P(d|q)=\frac{P(q|d)\times P(d)}{P(q)}
$$
对于同一个查询，同一篇文档的出现概率 $P(d),P(q)$都是固定的，因此只需计算条件概率$P(q|d)$
$$
P(d|q)=\frac{P(q|d)\times P(d)}{P(q)}
$$</li>
<li><strong>中文分词</strong></li>
</ol>
<h2 id="4马尔可夫模型">4、马尔可夫模型<a hidden class="anchor" aria-hidden="true" href="#4马尔可夫模型">#</a></h2>
<p>若状态到状态之间是有关联的，前一个状态有一定概率转移到到下一个状态，而多个状态之间的随机转移满足马尔科夫假设，那么这类随机过程就是一个马尔科夫随机过程。而刻画这类随机过程的统计模型，就是<strong>马尔科夫模型(Markov Model)</strong>。如果一个状态出现的概率只和前一个状态有关，则称它为<strong>一阶马尔科夫模型</strong>或者<strong>马尔科夫链</strong>。对应于三元、四元甚至更多元的文法，我们也有二阶、三阶等马尔科夫模型。</p>
<!-- raw HTML omitted -->
<h2 id="5信息熵">5、信息熵<a hidden class="anchor" aria-hidden="true" href="#5信息熵">#</a></h2>
<p><strong>信息熵(Entropy)</strong>，简称为熵，是用来刻画给定集合的<strong>纯净度</strong>的一个指标——集合里的元素全属于同一分组表示最纯净，即熵为0；若集合里的元素来自不同分组，则熵&gt;0。其具体的计算公式如下:
$$
Entropy(P)=-\sum_{i=1}^n (p_i\times \log_2p_i)
$$
其中，$n$表示集合中分组数量，$p_i$表示第$i$分组的元素出现在集合中的概率。</p>
<h3 id="51-信息量期望">5.1 信息量期望<a hidden class="anchor" aria-hidden="true" href="#51-信息量期望">#</a></h3>
<p>熵的公式是用来计算某个随机变量的<strong>信息量</strong>期望，而信息量是信息论中的一个度量，表示当观察到某个随机变量的具体值时，接收到了多少信息。而接收到的信息量跟发生事件的概率有关。事情发生概率越大，产生的信息量越小；反之则反。因此，要设计一个能够描述信息量的函数，需同时考虑到下面这三个特点：</p>
<ul>
<li>信息量应该为正数；</li>
<li>一个事件的信息量和它发生的概率成反比；</li>
<li>$H(x)$与$P(x)$的对数有关。$H(x)$表示 $x$ 的信息量，$P(x)$表示 $x$ 出现的概率。假设存在两个不相关的事件 $x$ 和 $y$，则观察到这两个事件同时发生时获得的信息量，应等于这两事件各自发生时的信息量之和，即$H(x,y)=H(x)+H(y)$。若 $x,y$ 是两个不相关的事件，那么就有 $P(x,y)=P(x)\times P(y)$。</li>
</ul>
<p>依据以上三点可设计出公式：$H(x)=-\log_2 P(x)$。<strong>可以使用其他大于1的数字作为对数的底，此处使用2只是约定俗成</strong>。这个公式可以量化随机变量某种取值时所产生的信息量。最后，加上计算随机变量不同概率所产生的信息量之期望，就得到了熵的公式。</p>
<p>单个集合的熵(信息量期望)如上，那包含多个小集合的大集合的熵，等于各个小集合之熵的加权平均，权重是小集合在整体中出现的概率：
$$
\sum_{v\in Value(T)} \frac{|P_v|}{|P|} Entropy(P_v)
$$
其中，$T$表示一种划分，$Pv$表示划分后其中某个小集合，$Entropy(P_v)$表示某个小集合的熵， 而$\frac{|Pv|}{|P|}$ 表示某个小集合出现的概率。</p>
<h3 id="52-信息增益information-gain">5.2 信息增益(Information Gain)<a hidden class="anchor" aria-hidden="true" href="#52-信息增益information-gain">#</a></h3>
<p>假设存在分组A、B、C，其出现的概率相等——均为$\frac{1}{3}$。那么当他们属于同一集合时，整体熵为$-3\times (\frac{1}{3}\times \log_2 \frac{1}{3})\approx 1.58$；若A、B属于同一集合，C为另外集合，则AB熵为1，C熵为0，设AB集合和C集合的出现概率均为0.5，则整体熵为$0.5<em>1+0.5</em>0=0.5$。</p>
<p>可以看出，划分后的整体熵要小于划分之前的整体熵。因为每次划分，都可能将不同分组的元素区分开来，从而降低划分后每个小集合的混乱程度，即熵。划分后整体熵的下降，也称为信息增益(Information Gain)。划分后整体熵下降越多，信息增益越大：
$$
Gain(P,T)=Entropy(P)-\sum_{v\in Value(T)} \frac{|P_v|}{|P|} Entropy(P_v)
$$
其中 $T$ 表示当前选择的特征，即集合；$Entropy(P)$表示选择特征 $T$ 前的熵；$Entropy(P_v)$表示特征 $T$ 取值为 $v$ 分组的熵。减号后面的部分表示选择 $T$ 做决策之后，各种取值加权平均后整体的熵。<strong>$Gain(P,T)$ 表示两个熵值之差，越大表示信息增益越多，就越应该选择这维特征 $T$。</strong></p>
<h2 id="6决策树">6、决策树<a hidden class="anchor" aria-hidden="true" href="#6决策树">#</a></h2>
<h3 id="61-信息熵与分类">6.1 信息熵与分类<a hidden class="anchor" aria-hidden="true" href="#61-信息熵与分类">#</a></h3>
<p>以如今常见的一类心理测试为例——比如“测测你是哪个武侠人物”——通过回答一些问题来得出回答者的某些特征(比如性别是男是女；智商是高是低)，从而得出测试者对应的武侠人物是哪位。</p>
<table>
<thead>
<tr>
<th style="text-align:center">武侠人物</th>
<th style="text-align:center">性别</th>
<th style="text-align:center">智商</th>
<th style="text-align:center">情商</th>
<th style="text-align:center">侠义</th>
<th style="text-align:center">个性</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">A</td>
<td style="text-align:center">男</td>
<td style="text-align:center">高</td>
<td style="text-align:center">高</td>
<td style="text-align:center">高</td>
<td style="text-align:center">开朗</td>
</tr>
<tr>
<td style="text-align:center">B</td>
<td style="text-align:center">男</td>
<td style="text-align:center">高</td>
<td style="text-align:center">高</td>
<td style="text-align:center">中</td>
<td style="text-align:center">拘谨</td>
</tr>
<tr>
<td style="text-align:center">C</td>
<td style="text-align:center">男</td>
<td style="text-align:center">高</td>
<td style="text-align:center">中</td>
<td style="text-align:center">低</td>
<td style="text-align:center">开朗</td>
</tr>
<tr>
<td style="text-align:center">D</td>
<td style="text-align:center">男</td>
<td style="text-align:center">高</td>
<td style="text-align:center">中</td>
<td style="text-align:center">中</td>
<td style="text-align:center">拘谨</td>
</tr>
<tr>
<td style="text-align:center">E</td>
<td style="text-align:center">男</td>
<td style="text-align:center">中</td>
<td style="text-align:center">高</td>
<td style="text-align:center">高</td>
<td style="text-align:center">开朗</td>
</tr>
<tr>
<td style="text-align:center">F</td>
<td style="text-align:center">女</td>
<td style="text-align:center">高</td>
<td style="text-align:center">高</td>
<td style="text-align:center">低</td>
<td style="text-align:center">开朗</td>
</tr>
<tr>
<td style="text-align:center">G</td>
<td style="text-align:center">女</td>
<td style="text-align:center">高</td>
<td style="text-align:center">中</td>
<td style="text-align:center">高</td>
<td style="text-align:center">拘谨</td>
</tr>
<tr>
<td style="text-align:center">H</td>
<td style="text-align:center">女</td>
<td style="text-align:center">高</td>
<td style="text-align:center">中</td>
<td style="text-align:center">高</td>
<td style="text-align:center">开朗</td>
</tr>
<tr>
<td style="text-align:center">I</td>
<td style="text-align:center">女</td>
<td style="text-align:center">高</td>
<td style="text-align:center">中</td>
<td style="text-align:center">低</td>
<td style="text-align:center">开朗</td>
</tr>
<tr>
<td style="text-align:center">J</td>
<td style="text-align:center">女</td>
<td style="text-align:center">中</td>
<td style="text-align:center">中</td>
<td style="text-align:center">中</td>
<td style="text-align:center">开朗</td>
</tr>
</tbody>
</table>
<p>在回答问题前，测试者对应的武侠人物是不可知的，因此所有被测者属于同一个集合。假设被测者的概率分布和这 10 位武侠人物的先验概率分布相同，根据熵的计算公式可得整体的熵为：$-5\times(0.1\times \log_2 0.1)\approx 3.32$。</p>
<ul>
<li>性别特征有两个分组，出现概率为 5:5 相等。
<ul>
<li>因此男女两个小集合的熵是为$-1\times (5\times 0.2\times log_2 0.2)\approx 2.32$。</li>
<li>故性别集合整体熵为$1.5<em>2.32+0.5</em>2.32=2.32$。</li>
<li>因此使用性格测试题后，信息增益是$3.32-2.32=1$。</li>
</ul>
</li>
<li>同理，智商也有了两个分组，出现概率为 高:中=8:2。
<ul>
<li>高智商小集合的熵是为$-1\times (8\times 0.125\times log_2 0.125)=3$；</li>
<li>中智商小集合的熵是为$-1\times (2\times 0.5\times log_2 0.5)=1$。</li>
<li>性别集合整体熵为$0.8<em>3+0.2</em>1=2.6$，故其信息增益是$3.32-2.6=0.72$。</li>
</ul>
</li>
<li>&hellip;&hellip;</li>
</ul>
<p>故可得如下表：</p>
<table>
<thead>
<tr>
<th style="text-align:center">武侠人物</th>
<th style="text-align:center">性别</th>
<th style="text-align:center">智商</th>
<th style="text-align:center">情商</th>
<th style="text-align:center">侠义</th>
<th style="text-align:center">个性</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">特征熵</td>
<td style="text-align:center">2.32</td>
<td style="text-align:center">2.60</td>
<td style="text-align:center">2.35</td>
<td style="text-align:center">1.74</td>
<td style="text-align:center">2.44</td>
</tr>
<tr>
<td style="text-align:center">信息增益</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0.72</td>
<td style="text-align:center">0.97</td>
<td style="text-align:center">1.58</td>
<td style="text-align:center">0.88</td>
</tr>
</tbody>
</table>
<p>当所有测试者都被分配到了相应的武侠人物名下，那么每个人物分组都是最纯净的，即熵值为0。故测试问卷的过程就转化为<strong>如何将熵从 3.32 下降到 0</strong>的过程。而影响熵下降的，便是信息增益(Information Gain)。因此<strong>每次选择问题时，都优先选择信息增益最高的问题，这样熵值下降便是最快的</strong>。</p>
<h3 id="62-决策树训练">6.2 决策树训练<a hidden class="anchor" aria-hidden="true" href="#62-决策树训练">#</a></h3>
<p>而这便是**决策树(Decision Tree)**训练的基本思想。如果将测试的问题变为机器学习中的特征，那么测试调查的步骤即可泛化为决策树构建树的步骤：</p>
<ol>
<li>根据集合中的样本分类，为每个集合计算信息熵，并通过全部集合的熵之加权平均，获得整个数据集的熵。注意，一开始集合只有一个，并且包含了所有的样本。</li>
<li>根据信息增益，计算每个特征的区分能力。挑选区分能力最强的特征，并对每个集合进行更细的划分。</li>
<li>有了新的划分之后，回到第一步，重复第一步和第二步，直到没有更多的特征，或者所有的样本都已经被分好类。</li>
</ol>
<p>不过有一点需要注意的是，案例中的每类武侠人物都只有一个样本。但在泛化的机器学习问题中，每个类型可能会对应多个样本。也就是说可以有很多个郭靖，且每个的属性/特征并不完全一致，但都是“郭靖”。正因如此，决策树通常只能把整体的熵值降低到一个较低的值，而无法完全降到 0。这也意味着，训练得到的决策树模型，常常无法完全准确地划分训练样本，只能求到一个近似的解。</p>
<h3 id="63-几种决策树算法的异同">6.3 几种决策树算法的异同<a hidden class="anchor" aria-hidden="true" href="#63-几种决策树算法的异同">#</a></h3>
<p>随着机器学习的快速发展，人们也提出了不少优化版的决策树。采用信息增益来构建决策树的算法被称为<a href="https://zh.wikipedia.org/wiki/ID3%E7%AE%97%E6%B3%95">ID3(Iterative Dichotomiser 3，迭代二叉树 3 代)</a>。但是它有一个缺点——会优先考虑具有较多取值的特征。</p>
<p>更多的取值会把数据样本划分为更多更小的分组，这样信息增益就会大幅上升。但是这样构建出来的树，很容易导致机器学习中的过拟合现象，不利于决策树对新数据的预测。</p>
<p>为了克服这个问题，人们又提出了一个改进版，<a href="https://zh.wikipedia.org/wiki/C4.5%E7%AE%97%E6%B3%95">C4.5算法</a>。这个算法使用<strong>信息增益率 Information Gain Ratio</strong>来替代信息增益，作为选择特征的标准，并降低决策树过拟合的程度。信息增益率通过引入一个被称作<strong>分裂信息Split Information</strong>的项来惩罚取值较多的特征，其公式如下：
$$
SplitInformation(P,T)=-\sum_{i=1}^n\frac{|P_i|}{|P|} \times \log_2\frac{|P_i|}{|P|}
$$
其中，训练数据集$P$通过属性$T$的属性值，划分为$n$个子数据集，$|P_i|$表示第$i$ 个子数据集中样本的数量，$|P|$表示划分之前数据集中样本总数量。</p>
<p>熵计算的时候考虑的是，集合内数据是否属于同一个类，因此即使集合数量很多，但是集合内的数据如果都是来自相同的分类（或分组），那么熵还是会很低。而这里的分裂信息是不同的，它只考虑子集的数量。如果某个特征取值很多，那么相对应的子集数量就越多，最终分裂信息的值就会越大。因此分裂信息可用来惩罚取值很多的特征。具体的计算公式如下：
$$
GainRatio(P,T)=\frac{Gain(P,T)}{SplitInformation(P,T)}
$$</p>
<p>另一种常见的决策树是** CART 算法**(Classification and Regression Trees，分类与回归树)。这种算法和 ID3、C4.5 相比，主要有两处不同：</p>
<ul>
<li>在分类时，CART 不再采用信息增益或信息增益率，而是采用<strong>基尼指数</strong>(Gini)来选择最好的特征，并进行数据的划分；</li>
<li>在 ID3 和 C4.5 决策树中，算法根据特征的属性值划分数据，可能会划分出多个组。而CART算法采用了二叉树，每次把数据切成两份，分别进入左、右子树。</li>
</ul>
<p>当然，CART 算法和 ID3、C4.5 也有类似的地方。首先，CART 中每一次迭代都会降低基尼指数，这类似于降低信息熵的过程。而且基尼指数也是描述纯度，与信息熵相似。计算每个集合的纯度的公式如下：
$$
Gini(P)=1-\sum_{i-1}^n p_i^2
$$
其中，$n$为集合$P$中所包含的不同分组（或分类）数量。集合$P$中包含分组越多，这个集合的基尼指数越高，纯度越低。</p>
<p>而整个数据集的基尼指数计算如下：
$$
Gini(P,T)=\sum_{j-1}^m p_j^2 \times Gini(P_j)
$$
其中，$n$为全集使用特征$T$划分后，所形成的子集数量。$P_j$为第$j$个集合。</p>
<h2 id="7特征选择feature-selection">7、特征选择(Feature Selection)<a hidden class="anchor" aria-hidden="true" href="#7特征选择feature-selection">#</a></h2>
<p>机器学习主要包括<strong>监督式学习</strong>(Supervised Learning)和<strong>非监督式学习</strong>(Unsupervised Learning)。监督式学习，是指通过训练资料学习并建立一个模型，并依此模型推测新的实例，主要包括分类(Classification)和回归(Regression)。无论是在监督学习还是非监督学习中，都可以使用特征选择。</p>
<p>机器学习的步骤主要包括<strong>数据的准备、特征工程、模型拟合、离线和在线测试</strong>。测试过程也许会产生新的数据，用于进一步提升模型。在这些处理中，特征工程是非常重要的一步。特征工程不止将原始特征到计算机数据的转化，还包括特征选择、缺失值的填补和异常值的去除等等。这其中非常重要的一步就是<strong>特征选择</strong>——大量的数据类型和维度的出现，会加大机器学习的难度，并影响最终的准确度。而特征选择尝试发掘和预定义任务相关的特征，同时过滤不必要的噪音特征。</p>
<h3 id="71-利用信息熵进行特征选择">7.1 利用信息熵进行特征选择<a hidden class="anchor" aria-hidden="true" href="#71-利用信息熵进行特征选择">#</a></h3>
<p>对于分类问题，一个好的特征选择，应该可以把那些对分类有价值的信息提取出来，而过滤掉那些对分类没有什么价值的信息。</p>
<p>什么是对分类有价值的特征——如果一个特征，经常只在某个或少数几个分类中出现，而很少在其他分类中出现，那么说明这个特征具有较强的区分力，它的出现很可能预示着整个数据属于某个分类的概率很高或很低。<!-- raw HTML omitted -->
而这可以使用信息熵来表示——如果熵值低，说明包含这个特征的数据只出现在少数分类中，对于分类的判断有价值。计算出每个特征所对应的数据集之熵，对特征按照熵值由低到高进行排序，挑选出排列靠前的特征。</p>
<p>$Df_i$来表示所有出现特征$f_i$的数据集合，集合一共包含了$n$个分类$C$，而$c_j$表示这$n$个分类中的第$j$个。然后根据$Df_i$中分类$C$的分布，可计算出熵。计算公式如下：
$$
-\sum_{j=1}^n P(c_j|Df_i)\times \log_2 P(c_j|Df_i)
$$
不过此方法的一个缺点就是，只考虑了单个特征出现时，对应数据的分类情况，而并没有考虑整个数据集的分类情况。比如，虽然出现<code>电影</code>这个词的文章，经常出现在<code>娱乐</code>这个分类中，很少出现在其他分类中，但是可能整个样本数据中，<code>娱乐</code>这个分类本来就已经占绝大多数，所以<code>电影</code>可能并非一个很有信息含量的特征。</p>
<p>为改进此，可转用<strong>信息增益</strong>的概念——若基于某个特征的划分所产生的信息增益越大，则特征对于分类的判断越有价值。把单个特征$f$是否出现作为一个决策条件，将数据集分为两类：</p>
<ul>
<li>$Df_i$表示出现了这个特征的数据，</li>
<li>$D\overline{f}_i$表示没有出现这个特征的数据。</li>
</ul>
<p>那么使用特征$f_i$进行数据划分之后，可得到基于两个新数据集的熵，然后和没有划分之前的熵进行比较，得出信息增益。
$$
-\sum_{j=1}^n P(c_j)\times \log_2 P(c_j)+P(f_i)\times \sum_{j=1}^n P(c_j|Df_i)\times \log_2 P(c_j|Df_i)+P(\overline{f}<em>i)\times \sum</em>{j=1}^n P(c_j|D\overline{f}_i)\times \log_2 P(c_j|D\overline{f}_i)
$$</p>
<h3 id="72-利用卡方检验进行特征选择">7.2 利用卡方检验进行特征选择<a hidden class="anchor" aria-hidden="true" href="#72-利用卡方检验进行特征选择">#</a></h3>
<p>在统计学中，<strong>卡方检验可用来检验两个变量是否相互独立</strong>。运用在特征选择，可以检验特征与分类这两个变量是否独立。若两者独立，证明特征和分类没有明显的相关性，特征对于分类来说没有提供足够的信息量，不是一个特征；反之，则是。为了检验独立性，卡方检验考虑了四种情况的概率：$P(fi,cj),P(\overline{f}_i,\overline{c}_j),P(fi,\overline{c}_j),P(\overline{f}_i,c_j)$。</p>
<p>通过计算公式：
$$
\frac{N\times (P(fi,cj)\times P(\overline{f}_i,\overline{c}_j)-P(fi,\overline{c}_j)\times P(\overline{f}_i))^2}{P(fi)\times P(\overline{f}_i)\times P(\overline{c}_j)\times P(c_j)}
$$
其中，$N$表示数据的总个数。如果一个特征和分类的相关性很高，无论是正向相关还是负向相关，那么正向相关和负向相关的差值就很大，<strong>最终计算的值就很高</strong>。</p>

			</div>
			<div class="post-copyright">
				<a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/">
	<img alt="知识共享许可协议" style="border-width:0" src="https://i.creativecommons.org/l/by-nc/4.0/88x31.png" />
</a>
<br />本作品采用
<a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/">知识共享署名-非商业性使用 4.0 国际许可协议</a>
进行许可。
<br />
转载时请附上原文链接以注明出处，图片在使用时请保留全部内容(可适当缩放)并附上图片所属的文章链接。若进行商业性使用，请先联系笔者获取许可。
			</div>
		</div>
		<div class="post-footer-wrapper">
			<hr class="hr-fade"/>
			<div class="post-comment-wrapper"><div id="gitalk-container"></div>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css" crossorigin="anonymous">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js" crossorigin="anonymous"></script>
<script>
  var gitalk = new Gitalk({
    id: '2021-04-14 22:17:24 \u002b0800 CST',
    title: '基础概率模型',
    clientID: '',
    clientSecret: '',
    repo: '',
    owner: '',
    admin: [''],
    body: decodeURI(location.href)
  });
  gitalk.render('gitalk-container');
</script>
<noscript>Please enable JavaScript to view the <a href="https://github.com/gitalk/gitalk">comments powered by gitalk.</a></noscript></div> 
		</div>
	</div>
</div>

</main>
	</body>
</html>
